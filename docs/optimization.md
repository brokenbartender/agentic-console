# Optimization Notes

This project supports local models via Ollama. Future production optimization paths:
- Quantization: use GGUF/GGML or Ollama quantized models
- Distillation: train smaller task models for low-latency tool routing
- Speculative decoding: use a small draft model + large verifier

These are placeholders aligned to the "Building Agentic AI" roadmap.
