# Benchmarks

- `evals/reasoning_bench.json`: small reasoning checks
- `evals/reasoning_bench.py`: optional model benchmark runner
- `evals/tool_selection.json`: tool-selection accuracy checks

Use these to compare models and detect tool-selection bias.
